# **HW4 Handout**

In this assignment, we are introducing a new format for assignment delivery, designed to enhance your development workflow. The key motivations for this change are:

- **Test Suite Integration**: Your code will be tested in a manner similar to `HWP1's`.
- **Local Development**: You will be able to perform most of your initial development locally, reducing the need for compute resources.
- **Hands-on Experience**: This assignment provides an opportunity to build an end-to-end deep learning pipeline from scratch. We will be substantially reducing the amount of abstractions compared to previous assignments.

For our provided notebook's to work, your notebook's current working directory must be the same as the handout.
This is important because the relative imports in the notebook's depend on the current working directory.
This can be achieved by:

1. Physically moving the notebook's into the handout directory.
2. Changing the notebook's current working directory to the handout directory using the `os.chdir()` function.

Your current working directory should have the following files for this assignment:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ hw4lib/
â”œâ”€â”€ mytorch/
â”œâ”€â”€ tests/
â”œâ”€â”€ hw4_data_subset/
â””â”€â”€ requirements.txt
```

## ğŸ“Š Dataset Structure

We have provided a subset of the dataset for you to use. This subset has been provided with the intention of allowing you to implement and test your code locally. The subset follows the same structure as the original dataset and is organized as follows:

```
hw4_data_subset/
â”œâ”€â”€ hw4p1_data/ # For causal language modeling
â”‚ â”œâ”€â”€ train/
â”‚ â”œâ”€â”€ valid/
â”‚ â””â”€â”€ test/
â””â”€â”€ hw4p2_data/ # For end-to-end speech recognition
â”œâ”€â”€ dev-clean/
â”‚ â”œâ”€â”€ fbank/
â”‚ â””â”€â”€ text/
â”œâ”€â”€ test-clean/
â”‚ â””â”€â”€ fbank/
â””â”€â”€ train-clean-100/
â”œâ”€â”€ fbank/
â””â”€â”€ text/

```

## ğŸ”§ Implementation Files

### Main Library (`hw4lib/`)

For `HW4P1` and `HW4P2`, you will incrementally implement components of `hw4lib` to build and train two models:

- **HW4P1**: A _Decoder-only Transformer_ for causal language modeling.
- **HW4P2**: An _Encoder-Decoder Transformer_ for end-to-end speech recognition.

Many of the components you implement will be reusable across both parts, reinforcing modular design and efficient implementation. You should see the following files in the `hw4lib/` directory (`__init__.py`'s are not shown):

```
hw4lib/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ tokenizer_jsons/
â”‚ â”œâ”€â”€ asr_dataset.py
â”‚ â”œâ”€â”€ lm_dataset.py
â”‚ â””â”€â”€ tokenizer.py
â”œâ”€â”€ decoding/
â”‚ â””â”€â”€ sequence_generator.py
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ masks.py
â”‚ â”œâ”€â”€ positional_encoding.py
â”‚ â”œâ”€â”€ speech_embedding.py
â”‚ â”œâ”€â”€ sublayers.py
â”‚ â”œâ”€â”€ decoder_layers.py
â”‚ â”œâ”€â”€ encoder_layers.py
â”‚ â””â”€â”€ transformers.py
â”œâ”€â”€ trainers/
â”‚  â”œâ”€â”€ base_trainer.py
â”‚  â”œâ”€â”€ asr_trainer.py
â”‚  â””â”€â”€ lm_trainer.py
â””â”€â”€ utils/
   â”œâ”€â”€ create_lr_scheduler.py
   â””â”€â”€ create_optimizer.py
```

### MyTorch Library Components (`mytorch/`)

In `HW4P1` and `HW4P2`, you will build and train Transformer models using PyTorchâ€™s `nn.MultiHeadAttention`. To deepen your understanding of its internals, you will also implement a custom `MultiHeadAttention` module from scratch as part of your `mytorch` library, designed to closely match the PyTorch interface. You should see the following files in the `mytorch/` directory:

```

mytorch/nn/
â”œâ”€â”€ activation.py
â”œâ”€â”€ linear.py
â”œâ”€â”€ scaled_dot_product_attention.py
â””â”€â”€ multi_head_attention.py

```

### Test Suite (`tests/`)

In `HW4P1` and `HW4P2`, you will be provided with a test suite that will be used to test your implementation. You should see the following files in the `tests/` directory:

```

tests/
â”œâ”€â”€ testing_framework.py
â”œâ”€â”€ test_mytorch*.py
â”œâ”€â”€ test_dataset*.py
â”œâ”€â”€ test_mask*.py
â”œâ”€â”€ test_positional_encoding.py
â”œâ”€â”€ test_sublayers*.py
â”œâ”€â”€ test_encoderlayers*.py
â”œâ”€â”€ test_decoderlayers*.py
â”œâ”€â”€ test_transformers\*.py
â”œâ”€â”€ test_hw4p1.py
â””â”€â”€ test_decoding.py

```

command to run:
```
cd D:\ahYen Workspace\ahYen Work\CMU_academic\MSCD_Y1_2425\11785-Intro to DL\IDL-HW4
python -m tests.test_mytorch
```

## Setup

Follow the setup instructions based on your preferred environment!

### Local

One of our key goals in designing this assignment is to allow you to complete most of the preliminary implementation work locally.  
We highly recommend that you **pass all tests locally** using the provided `hw4_data_subset` before moving to a GPU runtime.  
To do this, simply:

#### Step 1: Create a new conda environment

```bash
# Be sure to deactivate any active environments first
conda create -n hw4 python=3.12.4
```

#### Step 2: Activate the conda environment

```bash
conda activate hw4
```

#### Step 3: Install the dependencies using the provided `requirements.txt`

```bash
pip install --no-cache-dir --ignore-installed -r requirements.txt
```

#### Step 4: Ensure that your notebook is in the same working directory as the `Handout`

This can be achieved by:

1. Physically moving the notebook into the handout directory.
2. Changing the notebookâ€™s current working directory to the handout directory using the os.chdir() function.

#### Step 5: Open the notebook and select the newly created environment from the kernel selector.

If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ hw4lib/
â”œâ”€â”€ mytorch/
â”œâ”€â”€ tests/
â””â”€â”€ hw4_data_subset/
```

### Colab

#### Step 1: Get your handout

- See writeup for recommended approaches.

##### Example: My preferred approach

```python
import os

# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)
os.environ['GITHUB_TOKEN'] = "your-token"

GITHUB_USERNAME = "your-username"
REPO_NAME = "IDL-HW4"
TOKEN = os.environ.get("GITHUB_TOKEN")
repo_url = f"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git"
!git clone {repo_url}

## -------------

# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)
!cd {REPO_NAME} && git pull
```

#### Step 2: Install Dependencies

- `NOTE`: Your runtime will be restarted to ensure all dependencies are updated.
- `NOTE`: You will see a runtime crashed message, this was intentionally done. Simply move on to the next cell.

```python
%pip install --no-deps -r IDL-HW4/requirements.txt
import os
# NOTE: This will restart the your colab Python runtime (required)!
os.kill(os.getpid(), 9)
```

#### Step 3: Obtain Data

- `NOTE`: This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.

```bash
!curl -L -o /content/s25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/s25-hw4-data
!unzip -q -o /content/s25-hw4-data.zip -d /content/hw4_data
!rm -rf /content/s25-hw4-data.zip
!du -h --max-depth=2 /content/hw4_data
```

#### Step 4: Move to Handout Directory

```python
import os
os.chdir('IDL-HW4')
!ls
```

You must be within the handout directory for the library imports to work!

- `NOTE`: You may have to repeat running this command anytime you restart your runtime.
- `NOTE`: You can do a `pwd` to check if you are in the right directory.
- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.

If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ hw4lib/
â”œâ”€â”€ mytorch/
â”œâ”€â”€ tests/
â””â”€â”€ hw4_data_subset/
```

### Kaggle

While it is possible to run the notebook on Kaggle, we would recommend against it. This assignment is more resource intensive and may run slower on Kaggle.

#### Step 1: Get your handout

- See writeup for recommended approaches.

##### Example: My preferred approach

```python
import os

# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)
os.environ['GITHUB_TOKEN'] = "your-token"

GITHUB_USERNAME = "your-username"
REPO_NAME = "IDL-HW4"
TOKEN = os.environ.get("GITHUB_TOKEN")
repo_url = f"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git"
!git clone {repo_url}

## -------------

# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)
!cd {REPO_NAME} && git pull
```

#### Step 2: Install Dependencies

Simply set the `Environment` setting in the notebook to `Always use latest environment`. No need to install anything.

#### Step 3: Obtain Data

##### âš ï¸ Important: Kaggle Users

If you are using Kaggle, **do not manually download the data!** The dataset is large and may exceed your available disk space. Instead, follow these steps to add the dataset directly to your notebook:

1. Open your **Kaggle Notebook**.
2. Navigate to **Notebook â†’ Input**.
3. Click **Add Input**.
4. In the search bar, paste the following URL:

ğŸ‘‰ [https://www.kaggle.com/datasets/cmu11785/s25-hw4-data](https://www.kaggle.com/datasets/cmu11785/s25-hw4-data)

5. Click the **â• (plus sign)** to add the dataset to your notebook.

##### ğŸ“Œ Note:

This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.

#### Step 4: Move to Handout Directory

```python
import os
os.chdir('IDL-HW4')
!ls
```

You must be within the handout directory for the library imports to work!

- `NOTE`: You may have to repeat running this command anytime you restart your runtime.
- `NOTE`: You can do a `pwd` to check if you are in the right directory.
- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.

If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ hw4lib/
â”œâ”€â”€ mytorch/
â”œâ”€â”€ tests/
â””â”€â”€ hw4_data_subset/
```

### PSC

#### Step 1: Get your handout

- See writeup for recommended approaches.
- If you use Remote - SSH to connect to Bridges2, you can upload the handout to your project directory and work from there.

##### Example: My preferred approach

```python
import os

# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)
os.environ['GITHUB_TOKEN'] = "your-token"

GITHUB_USERNAME = "your-username"
REPO_NAME = "IDL-HW4"
TOKEN = os.environ.get("GITHUB_TOKEN")
repo_url = f"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git"
!git clone {repo_url}

## -------------

# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)
!cd {REPO_NAME} && git pull
```

#### Step 2: Setting Up Your Environment on Bridges2

For this homework, we are providing a shared Conda environment for the entire class. Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:

##### 1. SSH into Bridges2

```bash
ssh username@bridges2.psc.edu
```

##### 2. Navigate to your Project Directory

```bash
cd $PROJECT
```

##### 3. Load the Anaconda Module

```bash
module load anaconda3
```

##### 4. Activate the provided HW4 Environment

```bash
# First, deactivate any existing Conda environment
conda deactivate
conda activate /jet/home/psamal/hw_envs/idl_hw4
```

##### 5. Request a Compute Node

```bash
interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00
```

##### 6. Re-activate Environment

If your Conda environment was deactivated due to node allocation:

```bash
# First, deactivate any existing Conda environment
conda deactivate
conda activate /jet/home/psamal/hw_envs/idl_hw4
```

##### 7. Start Jupyter Notebook

```bash
jupyter notebook --no-browser --ip=0.0.0.0
```

##### 8. Connect to Jupyter Server

You can now use your prefered way of connecting to the Jupyter Server. Your options should be covered in the docs linked in post 558 @ piazza.

The following is my preferred way of connecting to the Jupyter Server:

###### 8.1 Connect in VSCode

I prefer uploading the notebook to PSC Bridges2 storage ($PROJECT directory) and then connecting to the Jupyter Server from there.

1. Use Remote - SSH to connect to Bridges2 and navigate to your project directory.

2. Upload the notebook to the project directory.

3. Open the notebook in VSCode.

4. Go to **Kernel** â†’ **Select Another Kernel** â†’ **Existing Jupyter Server**

5. Enter the URL of the Jupyter Server:`http://{hostname}:{port}/tree?token={token}`

- eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`

> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output.

#### Step 3: Get Data

- `NOTE`: This will download and unzip data for both `HW4P1` and `HW4P2`
- `NOTE`: We are using `$LOCAL`: the scratch storage on local disk on the node running a job to store out data.
  - Disk accesses are much faster than what you would get from `$PROJECT` storage
  - `IT IS NOT PERSISTENT`
- `NOTE`: Make sure you have completed the previous steps before running this cell.
- Read more about it PSC File Spaces [here](https://www.psc.edu/resources/bridges-2/user-guide#file-spaces).

```bash
!curl -L -o $LOCAL/s25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/s25-hw4-data
!unzip -q -o $LOCAL/s25-hw4-data.zip -d $LOCAL/hw4_data
!rm -rf $LOCAL/s25-hw4-data.zip
!du -h --max-depth=2 $LOCAL/hw4_data
```

#### Step 4: Move to Handout Directory

Depending on the way you are running your notebook, you may or may not need to run this cell. As long as you are within the handout directory for the library imports to work!

```python
# Move to the handout directory if you are not there already
import os
os.chdir('IDL-HW4')
!ls
```

- `NOTE`: You may have to repeat running this command anytime you restart your runtime.
- `NOTE`: You can do a `pwd` to check if you are in the right directory.
- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.

If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ hw4lib/
â”œâ”€â”€ mytorch/
â”œâ”€â”€ tests/
â””â”€â”€ hw4_data_subset/
```

---
## Notes for setup PSC:
# ä½¿ç”¨PSC Bridges2è¿›è¡ŒGPUè®¡ç®—çš„æ­¥éª¤æŒ‡å—

ä»¥ä¸‹æ˜¯åœ¨PSC Bridges2ä¸Šè®¾ç½®å’Œä½¿ç”¨GPUèµ„æºè¿›è¡Œæ·±åº¦å­¦ä¹ ä»»åŠ¡çš„å®Œæ•´æ­¥éª¤æŒ‡å—ã€‚

## ç¬¬ä¸€æ­¥ï¼šSSHç™»å½•å¹¶è®¾ç½®äº¤äº’å¼ä¼šè¯

**ç»ˆç«¯1ï¼š**
```bash
# 1. SSHç™»å½•åˆ°Bridges2
ssh hchia@bridges2.psc.edu

# 2. æ¿€æ´»é¢„è®¾çš„condaç¯å¢ƒ
conda activate /jet/home/psamal/hw_envs/idl_hw4

# 3. è¯·æ±‚äº¤äº’å¼GPUä¼šè¯
interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00

# 4. ç§»åŠ¨åˆ°ä½ çš„é¡¹ç›®ç›®å½•
# need to be conda in (activate /jet/home/psamal/hw_envs/idl_hw4) deactivate (base)
cd /ocean/projects/cis250019p/hchia/

# 5. å¯åŠ¨JupyteræœåŠ¡å™¨
jupyter notebook --no-browser --ip=0.0.0.0
```

æ­¤æ—¶ç»ˆç«¯ä¼šæ˜¾ç¤ºä¸€ä¸ªJupyteræœåŠ¡å™¨URLï¼Œå…¶ä¸­åŒ…å«tokenä¿¡æ¯ï¼Œç±»ä¼¼ï¼š
`http://v<017>.ib.bridges2.psc.edu:8888/tree?token=196273168a80b29fea12bc85e4967228a09ef76313e7a69c`

## ç¬¬äºŒæ­¥ï¼šè®¾ç½®SSHç«¯å£è½¬å‘

**ç»ˆç«¯2ï¼š**
```bash
# åœ¨æœ¬åœ°è®¡ç®—æœºæ‰“å¼€æ–°ç»ˆç«¯ï¼Œè®¾ç½®SSHç«¯å£è½¬å‘
# æ›¿æ¢v017ä¸ºå®é™…åˆ†é…çš„èŠ‚ç‚¹å
ssh -L 8888:v010.ib.bridges2.psc.edu:8888 hchia@bridges2.psc.edu

```
è¿™ä¸ªç»ˆç«¯çª—å£éœ€è¦ä¸€ç›´ä¿æŒæ‰“å¼€çŠ¶æ€ã€‚

## ç¬¬ä¸‰æ­¥ï¼šä¸Šä¼ æ–‡ä»¶ï¼ˆå¦‚éœ€ï¼‰

**ç»ˆç«¯3ï¼š**
```bash
# ä»æœ¬åœ°ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨ï¼ˆæ ¹æ®éœ€è¦ï¼‰
scp "æœ¬åœ°æ–‡ä»¶è·¯å¾„" ç”¨æˆ·å@bridges2.psc.edu:/ocean/projects/cis250019p/ç”¨æˆ·å/æ–‡ä»¶å
```

## ç¬¬å››æ­¥ï¼šè®¿é—®Jupyterå¹¶å¼€å§‹å·¥ä½œ

1. åœ¨æœ¬åœ°æµè§ˆå™¨ä¸­è®¿é—®ï¼š`http://localhost:8888/`
2. è¾“å…¥tokenï¼ˆä»ç»ˆç«¯1ä¸­å¤åˆ¶ï¼‰
3. æŒ‰ç…§notebookæŒ‡ç¤ºè¿è¡Œä»£ç ï¼š
   - å…‹éš†GitHubä»£ç åº“
   - ä¸‹è½½æ•°æ®é›†åˆ°`$LOCAL`ç›®å½•
   - å®ç°æ‰€éœ€ç»„ä»¶
   - è®­ç»ƒæ¨¡å‹
   - ç”Ÿæˆç»“æœå¹¶æäº¤

## é‡è¦æ³¨æ„äº‹é¡¹

1. **è·¯å¾„åŒºåˆ«ï¼š**
   - `$LOCAL`ï¼šä¸´æ—¶å­˜å‚¨ï¼Œé€Ÿåº¦å¿«ï¼Œä»»åŠ¡ç»“æŸååˆ é™¤
   - `/ocean/projects/...`ï¼šæŒä¹…åŒ–å­˜å‚¨ï¼Œé€‚åˆä»£ç å’Œç»“æœ

2. **èµ„æºé™åˆ¶ï¼š**
   - äº¤äº’å¼ä¼šè¯æœ‰æ—¶é—´é™åˆ¶ï¼ˆä¾‹å¦‚8å°æ—¶ï¼‰
   - åŠæ—¶ä¿å­˜å·¥ä½œ
   - è®°ä½tokenä¼šåœ¨æ¯æ¬¡é‡å¯Jupyteræ—¶æ”¹å˜

3. **æ“ä½œé¡ºåºï¼š**
   - ç»ˆç«¯1å¯åŠ¨JupyteræœåŠ¡å™¨
   - ç»ˆç«¯2å»ºç«‹ç«¯å£è½¬å‘ï¼ˆä¿æŒè¿è¡Œï¼‰
   - ç»ˆç«¯3ç”¨äºæ–‡ä»¶ä¼ è¾“ï¼ˆæŒ‰éœ€ï¼‰
   - æµè§ˆå™¨è®¿é—®æœ¬åœ°ç«¯å£`localhost:8888`

æŒ‰ç…§è¿™äº›æ­¥éª¤ï¼Œä½ å¯ä»¥å……åˆ†åˆ©ç”¨PSC Bridges2ä¸Šçš„GPUèµ„æºè¿›è¡Œæ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚