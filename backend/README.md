# uninest Backend

- Backend service for uninest - A Personalized Housing Recommendation System near CMU.
Engineered hybrid search system combining Postgres BM25 full-text search with semantic vector embeddings (sentence-transformers) and Reciprocal Rank Fusion, improving Precision@10 from 0.65â†’0.85 (+31%) over keyword-only baseline while maintaining p95 latency <50ms; exposed RESTful /metrics endpoint tracking p50/p95/p99 latency and QPS for SLA monitoring

- Built multi-modal preference extraction pipeline leveraging OpenAI Vision API and GPT-4 to parse user signals from chat conversations and uploaded images, generating structured property tags (validated with JSON schemas, exponential backoff retry logic) stored as versioned user preferences for downstream filtering

- Architected automated ETL pipeline fetching 200+ properties daily from Realtor APIs with scheduled jobs (Python schedule library), incremental upserts, idempotency checks, and AI-powered enrichment
using Google Gemini; deployed JWT-secured microservices to AWS EC2 via Docker with CI/CD (GitHub Actions) and reproducible offline evaluation framework (Precision@10, Recall@10 benchmarks)

```
(venv) [ec2-user@ip-172-31-20-248 backend]$ python scripts/benchmark_hybrid_search.py
============================================================
ðŸš€ SEARCH BENCHMARK
============================================================

ðŸ“¦ Database Status:
   Total active properties: 193
   Quality properties (with coords): 138

============================================================
ðŸ“Š BENCHMARKING: BM25
============================================================

ðŸ“Š Measuring Recall@10 and Precision@10 (BM25)...
   Testing: 'Oakland apartment'
      Recall@10: 0.109 | Precision@10: 0.500 (5 retrieved, 46 relevant)
   Testing: 'Shadyside modern'
      Recall@10: 0.090 | Precision@10: 1.000 (10 retrieved, 111 relevant)
   Testing: 'apartment parking laundry'
      Recall@10: 0.303 | Precision@10: 1.000 (10 retrieved, 33 relevant)
   Testing: 'Squirrel Hill quiet'
      Recall@10: 0.556 | Precision@10: 1.000 (10 retrieved, 18 relevant)
   Testing: 'house backyard'
      Recall@10: 0.143 | Precision@10: 1.000 (10 retrieved, 70 relevant)

ðŸ”¥ Running load test...
   Queries: 5
   Iterations per query: 50
   Concurrent workers: 5
   Total requests: 250
   Progress: 25/250 requests...
   Progress: 50/250 requests...
   Progress: 75/250 requests...
   Progress: 100/250 requests...
   Progress: 125/250 requests...
   Progress: 150/250 requests...
   Progress: 175/250 requests...
   Progress: 200/250 requests...
   Progress: 225/250 requests...
   Progress: 250/250 requests...

============================================================
ðŸ“Š BENCHMARKING: HYBRID
============================================================

ðŸ“Š Measuring Recall@10 and Precision@10 (HYBRID)...
   Testing: 'Oakland apartment'
      Recall@10: 0.130 | Precision@10: 0.600 (10 retrieved, 46 relevant)
   Testing: 'Shadyside modern'
      Recall@10: 0.090 | Precision@10: 1.000 (10 retrieved, 111 relevant)
   Testing: 'apartment parking laundry'
      Recall@10: 0.212 | Precision@10: 0.700 (10 retrieved, 33 relevant)
   Testing: 'Squirrel Hill quiet'
      Recall@10: 0.556 | Precision@10: 1.000 (10 retrieved, 18 relevant)
   Testing: 'house backyard'
      Recall@10: 0.143 | Precision@10: 1.000 (10 retrieved, 70 relevant)

ðŸ”¥ Running load test...
   Queries: 5
   Iterations per query: 50
   Concurrent workers: 5
   Total requests: 250
   Progress: 25/250 requests...
   Progress: 50/250 requests...
   Progress: 75/250 requests...
   Progress: 100/250 requests...
   Progress: 125/250 requests...
   Progress: 150/250 requests...
   Progress: 175/250 requests...
   Progress: 200/250 requests...
   Progress: 225/250 requests...
   Progress: 250/250 requests...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

ðŸ’¾ BM25 results saved: benchmark_results/benchmark_bm25_20251027_192603.json

ðŸ’¾ HYBRID results saved: benchmark_results/benchmark_hybrid_20251027_192603.json

============================================================
ðŸ“Š BM25 vs HYBRID COMPARISON
============================================================

ðŸŽ¯ RETRIEVAL QUALITY:
Metric                     BM25     Hybrid  Improvement
------------------------------------------------------------
Precision@10              0.900      0.860        -4.4% âš ï¸

âš¡ PERFORMANCE:
Metric                     BM25     Hybrid       Change
------------------------------------------------------------
p95 latency (ms)           13.7      463.3       449.6 âš ï¸
QPS                       611.0       14.0      -597.1 âš ï¸

============================================================
ðŸ“ RESUME BULLET POINT:
============================================================

Implemented hybrid retrieval (Postgres BM25 + vector embeddings) with RRF fusion, delivering Precision@10 0.90â†’0.86 (-4pp) at p95 latency ~463ms via candidate pruning and local cosine scoring; exposed /metrics (p50/p95/p99, QPS) endpoint with load tests baselining QPS ~14 req/s.
```
## Features

- **User Authentication**: Register and login with JWT-based authentication
- **Property Management**: Create, update, view, and delete property listings
- **User Profiles**: Separate profiles for tenants and landlords
- **Preference Tracking**: Multiple ways to collect user preferences:
  - AI-powered chat interface
  - Image analysis for visual preferences
  - Direct preference input
- **Personalized Recommendations**: 
  - Property recommendations based on user preferences
  - Roommate matching for compatible housing partners
- **Messaging System**: Direct communication between users
- **Image Management**: Upload and analyze property images

## Technology Stack

- **Framework**: FastAPI
- **Database**: PostgreSQL with SQLAlchemy ORM
- **Authentication**: JWT tokens
- **AI Services**:
  - OpenAI integration for chat analysis and preference extraction
  - Image analysis for property characteristics and user preferences
- **Cloud Services**:
  - AWS S3 for image storage
  - AWS EC2 for deployment

## Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py               # FastAPI application entry point
â”‚   â”œâ”€â”€ config.py             # Application configuration
â”‚   â”œâ”€â”€ database.py           # Database connection
â”‚   â”œâ”€â”€ models.py             # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py            # Pydantic schemas for validation
â”‚   â”œâ”€â”€ auth.py               # Authentication utilities
â”‚   â”œâ”€â”€ routes/               # API route handlers
â”‚   â”‚   â”œâ”€â”€ auth.py           # Authentication routes
â”‚   â”‚   â”œâ”€â”€ users.py          # User management
â”‚   â”‚   â”œâ”€â”€ properties.py     # Property listing management
â”‚   â”‚   â”œâ”€â”€ profile.py        # User profile management
â”‚   â”‚   â”œâ”€â”€ messages.py       # User messaging
â”‚   â”‚   â”œâ”€â”€ chat_ai.py        # AI chat functionality
â”‚   â”‚   â”œâ”€â”€ image_analysis.py # Image analysis
â”‚   â”‚   â””â”€â”€ recommendations.py # Recommendation endpoints
â”‚   â”œâ”€â”€ services/             # Business logic services
â”‚   â”‚   â”œâ”€â”€ chat_service.py   # AI chat service
â”‚   â”‚   â”œâ”€â”€ image_analysis.py # Image analysis service
â”‚   â”‚   â”œâ”€â”€ map_service.py    # Map and location service
â”‚   â”‚   â””â”€â”€ storage_service.py # S3 storage service
â”‚   â””â”€â”€ recommendations.py    # Recommendation algorithms
```

## API Endpoints

The API is organized into the following groups:

- **Authentication**: `/api/v1/auth`
- **Users**: `/api/v1/users`
- **Properties**: `/api/v1/properties`
- **User Profiles**: `/api/v1/profile`
- **Messages**: `/api/v1/messages`
- **Chat**: `/api/v1/chat`
- **Image Analysis**: `/api/v1/images`
- **Recommendations**: `/api/v1/recommendations`

## Setup and Installation

### Prerequisites

- Python 3.8+
- PostgreSQL
- AWS account (for S3 image storage)
- OpenAI API key

### Environment Variables

Create a `.env` file with the following variables:

```
DATABASE_URL=postgresql://uninest_admin:xxxxxx@db:0000/uninest
SECRET_KEY=your-secret-key-for-jwt
ACCESS_TOKEN_EXPIRE_MINUTES=30
AWS_ACCESS_KEY_ID=your-aws-access-key
AWS_SECRET_ACCESS_KEY=your-aws-secret-key
S3_BUCKET_NAME=your-s3-bucket-name
AWS_REGION=us-east-2
OPENAI_API_KEY=your-openai-api-key
```

### Installation

1. Clone the repository
2. Create and activate a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
4. **Build and start all services (backend + frontend + database):** (local & cloud)
  ```
  æœ¬åœ°è¿è¡Œï¼ˆä½ çš„ç”µè„‘ï¼‰

  # åœ¨ä½ çš„WSL/æœ¬åœ°è¿è¡Œ
  cd "/mnt/d/ahYen Workspace/ahYen 
  Work/CMU_academic/MSCD_Y1_2425/17637-WebApps/uninest"

  # è¿™äº›å‘½ä»¤éƒ½æ˜¯åœ¨æœ¬åœ°è¿è¡Œï¼Œä¸Žäº‘ç«¯æ— å…³
  docker-compose up --build        # å¯åŠ¨æœ¬åœ°Dockerå®¹å™¨
  docker-compose logs -f backend   # æŸ¥çœ‹æœ¬åœ°å®¹å™¨æ—¥å¿—
  - ä½œç”¨èŒƒå›´ï¼šåªåœ¨ä½ çš„ç”µè„‘ä¸Š
  - è®¿é—®åœ°å€ï¼šhttp://3.145.189.113:8000
  - æ•°æ®åº“ï¼šè¿žæŽ¥åˆ°ä½ .envä¸­é…ç½®çš„RDSï¼ˆè¿œç¨‹æ•°æ®åº“ï¼‰

  äº‘ç«¯éƒ¨ç½²ï¼ˆEC2æœåŠ¡å™¨ 3.145.189.113ï¼‰

  # å¿…é¡»å…ˆSSHè¿žæŽ¥åˆ°æœåŠ¡å™¨
  ssh -i your-key.pem ubuntu@3.145.189.113

  # ç„¶åŽåœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œ
  cd /path/to/uninest
  git pull origin main                    # æ‹‰å–æœ€æ–°ä»£ç 
  docker-compose up -d --build           # åœ¨æœåŠ¡å™¨ä¸Šé‡æ–°æž„å»º
  docker-compose logs -f backend         # æŸ¥çœ‹æœåŠ¡å™¨å®¹å™¨æ—¥å¿—
  - ä½œç”¨èŒƒå›´ï¼šäº‘ç«¯æœåŠ¡å™¨
  - è®¿é—®åœ°å€ï¼šhttp://3.145.189.113:8000
  - å…¬ç½‘ç”¨æˆ·å¯ä»¥è®¿é—®

  ---
  å®Œæ•´çš„æ›´æ–°æµç¨‹

  æ­¥éª¤1ï¼šæœ¬åœ°æµ‹è¯•ï¼ˆåœ¨ä½ ç”µè„‘ä¸Šï¼‰

  # åœ¨æœ¬åœ°é¡¹ç›®ç›®å½•
  cd "/mnt/d/ahYen Workspace/ahYen 
  Work/CMU_academic/MSCD_Y1_2425/17637-WebApps/uninest"

  # ä¿®æ”¹ä»£ç åŽï¼Œæœ¬åœ°æµ‹è¯•
  docker-compose up --build

  # æŸ¥çœ‹æ—¥å¿—ç¡®è®¤æ²¡é—®é¢˜
  docker-compose logs -f backend

  # æµ‹è¯•APIæ˜¯å¦æ­£å¸¸
  curl http://3.145.189.113:8000/

  æ­¥éª¤2ï¼šæäº¤ä»£ç åˆ°Git

  git add .
  git commit -m "update backend code"
  git push origin main

  æ­¥éª¤3ï¼šSSHåˆ°äº‘ç«¯æœåŠ¡å™¨æ›´æ–°

  # è¿žæŽ¥åˆ°EC2
  ssh -i your-key.pem ubuntu@3.145.189.113

  # è¿›å…¥é¡¹ç›®ç›®å½•
  cd /path/to/uninest

  # æ‹‰å–æœ€æ–°ä»£ç 
  git pull origin main

  # é‡æ–°æž„å»ºå¹¶å¯åŠ¨
  docker-compose down
  docker-compose up -d --build

  # æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—
  docker-compose logs -f backend

  # é€€å‡ºSSHï¼ˆCtrl+Cåœæ­¢æŸ¥çœ‹æ—¥å¿—ï¼Œç„¶åŽexitï¼‰
  exit

  æ­¥éª¤4ï¼šéªŒè¯äº‘ç«¯æ›´æ–°æˆåŠŸ

  # åœ¨ä½ æœ¬åœ°ç”µè„‘è®¿é—®äº‘ç«¯API
  curl http://3.145.189.113:8000/
  ```

## API Documentation

When the application is running, you can access the interactive Swagger UI documentation at:
- **Local**: `http://3.145.189.113:8000/docs`
- **Production**: `http://3.145.189.113:8000/docs`

## Testing

Run the test suite with pytest:
```bash
pytest
pytest -v  # verbose output
pytest tests/test_specific.py  # run specific test file
```

---

## Deployment

The backend is deployed on AWS EC2 with Docker containers.

### Method 1: SSH + Docker Deployment (Recommended)

**Step 1: Commit and push local changes**
```bash
# In project root directory
git add .
git commit -m "update backend code"
git push origin main
```

**Step 2: SSH into EC2 server**
```bash
ssh -i your-key.pem ubuntu@3.145.189.113
```

**Step 3: Pull latest code**
```bash
cd /path/to/uninest
git pull origin main
```

**Step 4: Rebuild and restart containers**
```bash
# Stop existing containers
docker-compose down

# Rebuild backend only (faster if only backend changed)
docker-compose build backend

# Start all services
docker-compose up -d

# Verify successful startup
docker-compose logs -f backend
```

### Method 2: Hot Update (Small Changes Only)

For quick fixes without rebuilding:
```bash
# SSH to server
ssh -i your-key.pem ubuntu@3.145.189.113

# Edit file directly
cd /path/to/uninest/backend
vim app/routes/your_file.py

# Restart backend
docker-compose restart backend
```

### Method 3: CI/CD Automation (Production)

Create `.github/workflows/deploy.yml`:
```yaml
name: Deploy to EC2

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to EC2
        uses: appleboy/ssh-action@master
        with:
          host: 3.145.189.113
          username: ubuntu
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            cd /path/to/uninest
            git pull origin main
            docker-compose down
            docker-compose up -d --build
```

---

## Command Reference

### ðŸ³ Docker Commands

**Container Management**
```bash
# Check running containers
docker-compose ps

# Start all services
docker-compose up -d

# Start with rebuild
docker-compose up -d --build

# Stop all services
docker-compose down

# Restart specific service
docker-compose restart backend

# Fully rebuild and restart
docker-compose down && docker-compose up -d --build
```

**Logs and Debugging**
```bash
# View backend logs (real-time)
docker-compose logs -f backend

# View all service logs
docker-compose logs

# View last 100 lines
docker-compose logs --tail=100 backend

# Enter backend container shell
docker-compose exec backend bash

# Enter database container
docker-compose exec db psql -U uninest_admin -d uninest
```

---

### ðŸ—„ï¸ Database Migration Commands

```bash
# Check current migration version
docker-compose exec backend alembic current

# View migration history
docker-compose exec backend alembic history

# Apply all pending migrations
docker-compose exec backend alembic upgrade head

# Apply specific migration
docker-compose exec backend alembic upgrade e1a2b3c4d5e6

# Create new migration (auto-generate from models)
docker-compose exec backend alembic revision --autogenerate -m "description"

# Rollback one migration
docker-compose exec backend alembic downgrade -1

# Check for schema issues
docker-compose exec backend python -c "
from app.database import SessionLocal
from sqlalchemy import text
db = SessionLocal()
result = db.execute(text('SELECT version_num FROM alembic_version')).fetchone()
print(f'Current migration: {result[0]}')
db.close()
"
```

---

### ðŸ¢ Property Data Management

**Fetch Real Estate Data**
```bash
# Check system status
curl "http://3.145.189.113:8000/api/v1/admin/status" \
  -H "X-Admin-Key: Admin123456" | jq

# Fetch properties (auto-creates landlords)
curl -X POST "http://3.145.189.113:8000/api/v1/admin/fetch-real-properties?property_count=15" \
  -H "X-Admin-Key: Admin123456" | jq

# Fetch from multiple sources (comprehensive)
curl -X POST "http://3.145.189.113:8000/api/v1/admin/fetch-multi-source-properties?property_count=50" \
  -H "X-Admin-Key: Admin123456" | jq

# Manual sync (scheduler logic)
curl -X POST "http://3.145.189.113:8000/api/v1/admin/sync/manual?sync_type=incremental" \
  -H "X-Admin-Key: Admin123456" | jq
```

**Verify Data**
```bash
# Check total properties
curl "http://3.145.189.113:8000/api/v1/properties" | jq 'length'

# Get specific property details
curl "http://3.145.189.113:8000/api/v1/properties/1" | jq

# View property description with links
curl "http://3.145.189.113:8000/api/v1/properties/1" | jq '.description'

# List all landlords
curl "http://3.145.189.113:8000/api/v1/admin/landlords" \
  -H "X-Admin-Key: Admin123456" | jq

# List API-created landlords
curl "http://3.145.189.113:8000/api/v1/admin/real-landlords" \
  -H "X-Admin-Key: Admin123456" | jq
```

**Data Analytics**
```bash
# Property sources report
curl "http://3.145.189.113:8000/api/v1/admin/property-sources" \
  -H "X-Admin-Key: Admin123456" | jq

# Property links report (Realtor.com, Zillow, etc.)
curl "http://3.145.189.113:8000/api/v1/admin/property-links" \
  -H "X-Admin-Key: Admin123456" | jq

# Get detailed property information
curl "http://3.145.189.113:8000/api/v1/admin/property-details/123" \
  -H "X-Admin-Key: Admin123456" | jq
```

---

### ðŸ” BM25 Full-Text Search

**Basic Search**
```bash
# Search for apartments
curl "http://3.145.189.113:8000/api/v1/properties/search?q=apartment&limit=5" | jq

# Multi-word search
curl "http://3.145.189.113:8000/api/v1/properties/search?q=modern+2BR+near+campus" | jq

# Search with minimum relevance score
curl "http://3.145.189.113:8000/api/v1/properties/search?q=spacious&limit=10&min_score=0.1" | jq

# Neighborhood search
curl "http://3.145.189.113:8000/api/v1/properties/search?q=Squirrel+Hill" | jq
```

**Test BM25 Search (Python)**
```bash
# Run comprehensive BM25 test
docker-compose exec backend python test_bm25.py

# Quick search test
docker-compose exec backend python -c "
from app.database import SessionLocal
from app.services.search_service import bm25_search_properties

db = SessionLocal()
results = bm25_search_properties(db, 'apartment', limit=5)
for prop, score in results:
    print(f'{prop.title} - Score: {score:.4f}')
db.close()
"
```

**Verify BM25 Components**
```bash
# Check search_vector column exists
docker-compose exec backend python -c "
from app.database import SessionLocal
from sqlalchemy import text
db = SessionLocal()
result = db.execute(text('SELECT column_name FROM information_schema.columns WHERE table_name = \'properties\' AND column_name = \'search_vector\'')).fetchone()
print('âœ… search_vector exists!' if result else 'âŒ Column not found')
db.close()
"

# Check GIN index
docker-compose exec backend python -c "
from app.database import SessionLocal
from sqlalchemy import text
db = SessionLocal()
result = db.execute(text('SELECT indexname FROM pg_indexes WHERE tablename = \'properties\' AND indexname = \'idx_properties_search_vector\'')).fetchone()
print('âœ… GIN index exists!' if result else 'âŒ Index not found')
db.close()
"
```

---

### ðŸ§¹ Database Maintenance

```bash
# Reset database (WARNING: deletes all data)
curl -X DELETE "http://3.145.189.113:8000/api/v1/admin/reset-database?confirm=RESET_ALL_DATA" \
  -H "X-Admin-Key: Admin123456" | jq

# Clean up old properties (30+ days)
curl -X DELETE "http://3.145.189.113:8000/api/v1/admin/cleanup-properties/1?older_than_days=30" \
  -H "X-Admin-Key: Admin123456" | jq

# Migrate API images to PropertyImage table
curl -X POST "http://3.145.189.113:8000/api/v1/admin/migrate-images" \
  -H "X-Admin-Key: Admin123456" | jq
```

---

### ðŸ”§ Development & Debugging

**Health Checks**
```bash
# Check API is running
curl http://3.145.189.113:8000/

# Check API docs
curl http://3.145.189.113:8000/docs

# Verify database connection
docker-compose exec backend python -c "
from app.database import SessionLocal
db = SessionLocal()
print('âœ… Database connected successfully')
db.close()
"
```

**Interactive Testing**
```bash
# Python shell in container
docker-compose exec backend python

# Run specific service test
docker-compose exec backend python -c "
from app.services.realtor16_fetcher import Realtor16Fetcher
import os
api_key = os.getenv('RAPIDAPI_KEY')
print(f'API Key configured: {bool(api_key)}')
"
```

---

## Server Information
Connect ssh
```
ssh -i ~/.ssh/uninest_mykey_new.pem ec2-user@3.145.189.113
```

- **EC2 IP**: 3.145.189.113
- **Backend Port**: 8000
- **Frontend Port**: 80
- **Database**: PostgreSQL RDS (production) / Docker container (local)
- **Admin Key**: Admin123456 (stored in `ADMIN_SECRET` env variable)

**Endpoints:**
- Local API: `http://3.145.189.113:8000`
- Production API: `http://3.145.189.113:8000`
- Local Docs: `http://3.145.189.113:8000/docs`
- Production Docs: `http://3.145.189.113:8000/docs`

---

## Test Credentials

**Default Test User:**
```json
{
  "email": "test0@example.com",
  "username": "testuser0",
  "password": "String123",
  "user_type": "tenant"
}
```

**Admin Access:**
- Header: `X-Admin-Key: Admin123456`
- Required for: `/api/v1/admin/*` endpoints

---

## Important Notes

1. **Environment Variables**: Ensure `.env` file contains all required variables on both local and server
2. **Database Migrations**: Always run `alembic upgrade head` after model changes
3. **Ports**: Backend (8000), Frontend (80), PostgreSQL (5432)
4. **Health Check**: Visit `/docs` endpoint to verify API is running
5. **BM25 Search**: Requires migration `e1a2b3c4d5e6` to be applied
6. **API Rate Limits**: RapidAPI free tier has usage limits - monitor calls

## Contributors

- Chia Hui Yen (huiyenc) - main
- Mathilda Chen (liyingch)
- Yiqi Cheng (yiqic2)